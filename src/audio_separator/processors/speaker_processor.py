"""
pyannote-audioè©±è€…åˆ†é›¢ãƒ—ãƒ­ã‚»ãƒƒã‚µ

pyannote-audioãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è©±è€…ã®è­˜åˆ¥ãƒ»åˆ†é›¢ã‚’è¡Œã†å‡¦ç†ã‚’æä¾›
"""

import os
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any, Callable
import numpy as np

from ..utils.audio_utils import AudioUtils
from ..utils.file_utils import FileUtils


class SpeakerSegment:
    """è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, start_time: float, end_time: float, speaker_id: str, confidence: float = 1.0):
        """
        è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆæœŸåŒ–
        
        Args:
            start_time: é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
            end_time: çµ‚äº†æ™‚é–“ï¼ˆç§’ï¼‰
            speaker_id: è©±è€…ID
            confidence: ä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰
        """
        self.start_time = start_time
        self.end_time = end_time
        self.speaker_id = speaker_id
        self.confidence = confidence
    
    @property
    def duration(self) -> float:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ï¼ˆç§’ï¼‰"""
        return self.end_time - self.start_time
    
    def __repr__(self) -> str:
        return f"SpeakerSegment(speaker={self.speaker_id}, {self.start_time:.2f}-{self.end_time:.2f}s, conf={self.confidence:.2f})"


class SpeakerProcessor:
    """pyannote-audioè©±è€…åˆ†é›¢ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚¯ãƒ©ã‚¹"""
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
    AVAILABLE_MODELS = {
        'pyannote/speaker-diarization-3.1': {
            'description': 'pyannote-audio v3.1 è©±è€…åˆ†é›¢ãƒ¢ãƒ‡ãƒ«',
            'language_support': 'multilingual',
            'quality': 'high',
            'speed': 'medium'
        },
        'pyannote/speaker-diarization': {
            'description': 'pyannote-audio æ¨™æº–è©±è€…åˆ†é›¢ãƒ¢ãƒ‡ãƒ«',
            'language_support': 'multilingual', 
            'quality': 'standard',
            'speed': 'fast'
        }
    }
    
    def __init__(
        self,
        model_name: str = 'pyannote/speaker-diarization-3.1',
        device: str = 'auto',
        use_auth_token: bool = True
    ):
        """
        è©±è€…åˆ†é›¢ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’åˆæœŸåŒ–
        
        Args:
            model_name: ä½¿ç”¨ã™ã‚‹pyannoteãƒ¢ãƒ‡ãƒ«å
            device: å‡¦ç†ãƒ‡ãƒã‚¤ã‚¹ ('auto', 'cpu', 'cuda')
            use_auth_token: Hugging Faceèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        self.model_name = model_name
        self.device = device
        self.use_auth_token = use_auth_token
        self.pipeline = None
        self._is_initialized = False
        
        # ãƒ¢ãƒ‡ãƒ«åã®æ¤œè¨¼
        if model_name not in self.AVAILABLE_MODELS:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«: {model_name}")
        
        logging.info(f"è©±è€…åˆ†é›¢ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–: ãƒ¢ãƒ‡ãƒ«={model_name}, ãƒ‡ãƒã‚¤ã‚¹={device}")
    
    def _get_auth_token(self):
        """Hugging Faceèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        import os
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN')
        
        if token:
            logging.info("âœ… Hugging Face tokenãŒç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã•ã‚Œã¾ã—ãŸ")
            return token
        elif self.use_auth_token:
            logging.info("ğŸ”‘ Hugging Face tokenãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€ç’°å¢ƒå¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return True  # huggingface_hubã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨ã‚’è©¦è¡Œ
        else:
            logging.warning("âŒ Hugging Face tokenãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
    
    def _initialize_pipeline(self) -> None:
        """
        pyannote-audioãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ï¼ˆé…å»¶åˆæœŸåŒ–ï¼‰
        """
        if self._is_initialized:
            return
        
        try:
            # å®Ÿéš›ã®pyannote-audioã‚’è©¦è¡Œã€å¤±æ•—æ™‚ã¯ç°¡æ˜“è©±è€…åˆ†é›¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                from pyannote.audio import Pipeline
                import torch
                
                logging.info(f"pyannote-audioãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ '{self.model_name}' ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                
                # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆè¨­å®šã«åŸºã¥ãï¼‰
                if self.device == 'cuda':
                    # GPUå¼·åˆ¶ä½¿ç”¨
                    if torch.cuda.is_available():
                        device = torch.device('cuda')
                        logging.info("pyannote-audio: GPUä½¿ç”¨ã‚’å¼·åˆ¶ã—ã¦ã„ã¾ã™")
                    else:
                        logging.warning("pyannote-audio: GPUå¼·åˆ¶æŒ‡å®šã•ã‚Œã¾ã—ãŸãŒã€CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã§å®Ÿè¡Œã—ã¾ã™")
                        device = torch.device('cpu')
                elif self.device == 'cpu':
                    # CPUå¼·åˆ¶ä½¿ç”¨
                    device = torch.device('cpu')
                    logging.info("pyannote-audio: CPUä½¿ç”¨ã‚’å¼·åˆ¶ã—ã¦ã„ã¾ã™")
                else:
                    # auto: GPUå„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯CPU
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    logging.info(f"pyannote-audio: è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹é¸æŠ: {device}")
                
                # Hugging Face tokenè¨­å®šã®ç¢ºèªãƒ»è‡ªå‹•è¨­å®š
                auth_token = self._get_auth_token()
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
                self.pipeline = Pipeline.from_pretrained(
                    self.model_name,
                    use_auth_token=auth_token
                )
                
                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è©³ç´°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
                # ã‚ˆã‚Šç²¾å¯†ãªè©±è€…åˆ†é›¢ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
                if hasattr(self.pipeline, '_segmentation'):
                    # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ„Ÿåº¦èª¿æ•´ï¼ˆã‚ˆã‚Šç´°ã‹ã„åˆ†é›¢ï¼‰
                    if hasattr(self.pipeline._segmentation, 'onset'):
                        self.pipeline._segmentation.onset = 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ã‹ã‚‰ä¸‹ã’ã‚‹
                    if hasattr(self.pipeline._segmentation, 'offset'):
                        self.pipeline._segmentation.offset = 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ã‹ã‚‰ä¸‹ã’ã‚‹
                
                if hasattr(self.pipeline, '_clustering'):
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¾å€¤èª¿æ•´ï¼ˆè©±è€…ã‚’ã‚ˆã‚Šç´°ã‹ãåˆ†é›¢ï¼‰
                    if hasattr(self.pipeline._clustering, 'threshold'):
                        self.pipeline._clustering.threshold = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.7ã‹ã‚‰ä¸‹ã’ã‚‹
                
                # é‡è¤‡ç™ºè©±åˆ¶å¾¡ã®è¨­å®š
                if hasattr(self.pipeline, 'instantiate'):
                    # é‡è¤‡ç™ºè©±æ¤œå‡ºã‚’ç„¡åŠ¹åŒ–ï¼ˆé‡ãªã‚Šã‚’é¿ã‘ã‚‹ï¼‰
                    try:
                        # pyannote v3.1ã§ã¯é‡è¤‡éƒ¨åˆ†ã‚’è‡ªå‹•ã§å‡¦ç†ã™ã‚‹è¨­å®š
                        self.pipeline.instantiate({
                            'segmentation': {
                                'min_duration_on': 0.0,
                                'min_duration_off': 0.0
                            },
                            'clustering': {
                                'method': 'centroid',
                                'min_cluster_size': 2,
                                'threshold': 0.5
                            }
                        })
                    except Exception as e:
                        logging.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©³ç´°è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                
                # é‡è¤‡ç™ºè©±ã‚’é¿ã‘ã‚‹ãŸã‚ã®å¾Œå‡¦ç†ãƒ•ãƒ©ã‚°
                self._remove_overlapping_segments = True
                
                # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
                self.pipeline = self.pipeline.to(device)
                self._device = device
                self._pyannote_available = True
                
                logging.info(f"pyannote-audioãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº† (ãƒ‡ãƒã‚¤ã‚¹: {device})")
                
            except Exception as pyannote_error:
                logging.warning(f"pyannote-audioåˆ©ç”¨ä¸å¯: {pyannote_error}")
                logging.info("ç°¡æ˜“è©±è€…åˆ†é›¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                self.pipeline = f"simple_{self.model_name}"
                self._pyannote_available = False
            
            self._is_initialized = True
            
        except Exception as e:
            raise RuntimeError(f"è©±è€…åˆ†é›¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
    
    def _initialize_pipeline_with_params(
        self, 
        clustering_threshold: float,
        segmentation_onset: float,
        segmentation_offset: float
    ) -> None:
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«è¨­å®šã—ã¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–
        """
        # åŸºæœ¬åˆæœŸåŒ–
        self._initialize_pipeline()
        
        if hasattr(self, '_pyannote_available') and self._pyannote_available:
            try:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‹•çš„ã«æ›´æ–°
                if hasattr(self.pipeline, '_segmentation'):
                    if hasattr(self.pipeline._segmentation, 'onset'):
                        self.pipeline._segmentation.onset = segmentation_onset
                    if hasattr(self.pipeline._segmentation, 'offset'):
                        self.pipeline._segmentation.offset = segmentation_offset
                
                if hasattr(self.pipeline, '_clustering'):
                    if hasattr(self.pipeline._clustering, 'threshold'):
                        self.pipeline._clustering.threshold = clustering_threshold
                
                logging.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°å®Œäº†: clustering={clustering_threshold}, onset={segmentation_onset}, offset={segmentation_offset}")
                
            except Exception as e:
                logging.warning(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    
    def diarize(
        self,
        audio_path: str,
        min_duration: float = 0.5,  # ã‚ˆã‚ŠçŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚‚æ¤œå‡º
        max_speakers: Optional[int] = None,
        clustering_threshold: float = 0.5,  # ã‚ˆã‚Šç´°ã‹ãåˆ†é›¢
        segmentation_onset: float = 0.3,  # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ„Ÿåº¦
        segmentation_offset: float = 0.3,  # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ„Ÿåº¦
        force_num_speakers: Optional[int] = None  # å¼·åˆ¶çš„ã«æŒ‡å®šã—ãŸè©±è€…æ•°ã«åˆ†é›¢
    ) -> List[SpeakerSegment]:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è©±è€…åˆ†é›¢ã‚’å®Ÿè¡Œ
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            min_duration: æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆç§’ï¼‰
            max_speakers: æœ€å¤§è©±è€…æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰
            clustering_threshold: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¾å€¤ï¼ˆ0.1-1.0ã€ä½ã„ã»ã©ç´°ã‹ãåˆ†é›¢ï¼‰
            segmentation_onset: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹æ„Ÿåº¦ï¼ˆ0.1-0.9ã€ä½ã„ã»ã©ç´°ã‹ãæ¤œå‡ºï¼‰
            segmentation_offset: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ„Ÿåº¦ï¼ˆ0.1-0.9ã€ä½ã„ã»ã©ç´°ã‹ãæ¤œå‡ºï¼‰
            force_num_speakers: å¼·åˆ¶çš„ã«æŒ‡å®šã—ãŸè©±è€…æ•°ã«åˆ†é›¢ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰
            
        Returns:
            List[SpeakerSegment]: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Raises:
            FileNotFoundError: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            RuntimeError: è©±è€…åˆ†é›¢å‡¦ç†ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        audio_path = Path(audio_path)
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        if not AudioUtils.validate_audio_file(audio_path):
            raise FileNotFoundError(f"æœ‰åŠ¹ãªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
        
        logging.info(f"è©±è€…åˆ†é›¢é–‹å§‹: {audio_path}")
        logging.info(f"ãƒ¢ãƒ‡ãƒ«: {self.model_name}")
        logging.info(f"æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·: {min_duration}ç§’")
        
        try:
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ï¼‰
            self._initialize_pipeline_with_params(clustering_threshold, segmentation_onset, segmentation_offset)
            
            # éŸ³å£°æƒ…å ±å–å¾—
            audio_info = AudioUtils.get_audio_info(audio_path)
            duration = audio_info['duration']
            
            logging.info(f"éŸ³å£°é•·: {duration:.2f}ç§’")
            logging.info(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¾å€¤: {clustering_threshold}")
            logging.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ„Ÿåº¦: onset={segmentation_onset}, offset={segmentation_offset}")
            if force_num_speakers:
                logging.info(f"å¼·åˆ¶è©±è€…æ•°: {force_num_speakers}äºº")
            
            # BGMåˆ†é›¢å¾ŒéŸ³å£°ã®å“è³ªã‚’ç¢ºèª
            audio_info = AudioUtils.get_audio_info(audio_path)
            logging.info(f"è©±è€…åˆ†é›¢å…¥åŠ›éŸ³å£°å“è³ª: {audio_info}")
            
            # BGMåˆ†é›¢å¾Œã®éŸ³å£°ã«æœ€é©åŒ–ã•ã‚ŒãŸå‰å‡¦ç†ã‚’é©ç”¨
            if hasattr(self, '_pyannote_available') and self._pyannote_available:
                logging.info("BGMåˆ†é›¢æ¸ˆã¿éŸ³å£°ç”¨ã®è»½å¾®ãªå‰å‡¦ç†å®Ÿè¡Œä¸­...")
                
                # å…ƒéŸ³å£°ã‚’èª­ã¿è¾¼ã¿
                audio_data, sample_rate = AudioUtils.load_audio(audio_path)
                
                # BGMåˆ†é›¢å¾ŒéŸ³å£°ç”¨ã®è»½å¾®ãªå‡¦ç†ï¼ˆãƒã‚¤ã‚ºé™¤å»ã®ã¿ï¼‰
                enhanced_audio = AudioUtils.light_enhance_for_diarization(audio_data, sample_rate)
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                temp_enhanced_path = audio_path.parent / f"temp_light_enhanced_{audio_path.name}"
                AudioUtils.save_audio(enhanced_audio, temp_enhanced_path, sample_rate)
                
                # å‡¦ç†å¾Œã®éŸ³å£°ãƒ‘ã‚¹ã‚’æ›´æ–°
                audio_path_for_processing = temp_enhanced_path
                logging.info("è»½å¾®ãªå‰å‡¦ç†å®Œäº†")
            else:
                audio_path_for_processing = audio_path
            
            # è©±è€…åˆ†é›¢å®Ÿè¡Œ
            if hasattr(self, '_pyannote_available') and self._pyannote_available:
                segments = self._diarize_pyannote(audio_path_for_processing, duration, min_duration, max_speakers, clustering_threshold, force_num_speakers)
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                if audio_path_for_processing != audio_path:
                    try:
                        audio_path_for_processing.unlink()
                        logging.info("ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†")
                    except Exception:
                        pass
            else:
                segments = self._diarize_simple(audio_path, duration, min_duration, max_speakers, force_num_speakers)
            
            # çµæœã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_segments = [
                seg for seg in segments 
                if seg.duration >= min_duration
            ]
            
            logging.info(f"è©±è€…åˆ†é›¢å®Œäº†: {len(filtered_segments)}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ¤œå‡º")
            
            # è©±è€…çµ±è¨ˆ
            speakers = list(set(seg.speaker_id for seg in filtered_segments))
            for speaker in speakers:
                speaker_segments = [seg for seg in filtered_segments if seg.speaker_id == speaker]
                total_duration = sum(seg.duration for seg in speaker_segments)
                logging.info(f"è©±è€…{speaker}: {len(speaker_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ, åˆè¨ˆ{total_duration:.2f}ç§’")
            
            return filtered_segments
            
        except Exception as e:
            logging.error(f"è©±è€…åˆ†é›¢å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise RuntimeError(f"è©±è€…åˆ†é›¢ã«å¤±æ•—: {e}")
    
    def _diarize_pyannote(
        self,
        audio_path: Path,
        duration: float,
        min_duration: float,
        max_speakers: Optional[int],
        clustering_threshold: float = 0.7,
        force_num_speakers: Optional[int] = None
    ) -> List[SpeakerSegment]:
        """
        pyannote-audioã‚’ä½¿ç”¨ã—ãŸå®Ÿéš›ã®è©±è€…åˆ†é›¢
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            duration: éŸ³å£°ã®é•·ã•
            min_duration: æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·
            max_speakers: æœ€å¤§è©±è€…æ•°
            
        Returns:
            List[SpeakerSegment]: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        """
        logging.info("å®Ÿéš›ã®pyannote-audioã«ã‚ˆã‚‹è©±è€…åˆ†é›¢å®Ÿè¡Œä¸­...")
        
        try:
            # PyTorchã®Warningã‚’ä¸€æ™‚çš„ã«æŠ‘åˆ¶
            import warnings
            warnings.filterwarnings("ignore", message="std(): degrees of freedom is <= 0")
            # pyannote-audioãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
            # v3.1ã§ã¯ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ã“ã¨ãŒã§ããªã„ãŸã‚ã€æ¨™æº–å®Ÿè¡Œ
            diarization = self.pipeline(str(audio_path))
            
            # çµæœã‚’SpeakerSegmentã«å¤‰æ›
            segments = []
            for turn, track, speaker in diarization.itertracks(yield_label=True):
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ãƒã‚§ãƒƒã‚¯
                segment_duration = turn.end - turn.start
                if segment_duration >= min_duration:
                    segment = SpeakerSegment(
                        start_time=turn.start,
                        end_time=turn.end,
                        speaker_id=speaker,
                        confidence=1.0  # pyannoteã¯ä¿¡é ¼åº¦ã‚’ç›´æ¥æä¾›ã—ãªã„ãŸã‚1.0ã¨ã™ã‚‹
                    )
                    segments.append(segment)
            
            # è©±è€…æ•°åˆ¶é™å‡¦ç†ï¼ˆå¼·åˆ¶è©±è€…æ•°ã‚’å„ªå…ˆï¼‰
            target_speakers = force_num_speakers if force_num_speakers is not None else max_speakers
            
            if target_speakers is not None:
                unique_speakers = len(set(seg.speaker_id for seg in segments))
                if force_num_speakers is not None:
                    logging.info(f"æ¤œå‡ºã•ã‚ŒãŸè©±è€…æ•°: {unique_speakers}äºº, å¼·åˆ¶è©±è€…æ•°: {force_num_speakers}äºº")
                else:
                    logging.info(f"æ¤œå‡ºã•ã‚ŒãŸè©±è€…æ•°: {unique_speakers}äºº, æœ€å¤§è©±è€…æ•°: {max_speakers}äºº")
                
                if unique_speakers > target_speakers:
                    # æ¤œå‡ºã•ã‚ŒãŸè©±è€…æ•°ãŒå¤šã„å ´åˆã€ä¸Šä½Näººã«åˆ¶é™
                    if force_num_speakers is not None:
                        logging.info(f"è©±è€…æ•°ã‚’{unique_speakers}äººã‹ã‚‰{force_num_speakers}äººã«åˆ¶é™ã—ã¾ã™")
                    else:
                        logging.info(f"è©±è€…æ•°ã‚’{unique_speakers}äººã‹ã‚‰{max_speakers}äººã«åˆ¶é™ã—ã¾ã™")
                    
                    # è©±è€…ã”ã¨ã®ç·ç™ºè©±æ™‚é–“ã‚’è¨ˆç®—
                    speaker_durations = {}
                    for segment in segments:
                        if segment.speaker_id not in speaker_durations:
                            speaker_durations[segment.speaker_id] = 0.0
                        speaker_durations[segment.speaker_id] += segment.duration
                    
                    # ç™ºè©±æ™‚é–“ã®é•·ã„é †ã«ã‚½ãƒ¼ãƒˆ
                    top_speakers = sorted(
                        speaker_durations.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )[:target_speakers]
                    
                    top_speaker_ids = [speaker_id for speaker_id, _ in top_speakers]
                    
                    # ä¸Šä½è©±è€…ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’ä¿æŒ
                    segments = [seg for seg in segments if seg.speaker_id in top_speaker_ids]
                    
                    if force_num_speakers is not None:
                        logging.info(f"å¼·åˆ¶è©±è€…æ•°åˆ¶é™é©ç”¨å®Œäº†: {len(top_speaker_ids)}äººã®è©±è€…ã‚’ä¿æŒ")
                    else:
                        logging.info(f"æœ€å¤§è©±è€…æ•°åˆ¶é™é©ç”¨å®Œäº†: {len(top_speaker_ids)}äººã®è©±è€…ã‚’ä¿æŒ")
                
                elif force_num_speakers is not None and unique_speakers == 1 and force_num_speakers > 1:
                    # 1ã¤ã®è©±è€…ã—ã‹æ¤œå‡ºã•ã‚Œãªã‹ã£ãŸå ´åˆã®åˆ†å‰²å‡¦ç†
                    logging.info(f"è©±è€…æ•°ãŒ{unique_speakers}äººã®ãŸã‚ã€{force_num_speakers}äººã«å¼·åˆ¶åˆ†å‰²ã—ã¾ã™")
                    segments = self._force_split_speakers(segments, force_num_speakers)
            
            # é‡è¤‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»å‡¦ç†
            if hasattr(self, '_remove_overlapping_segments') and self._remove_overlapping_segments:
                segments = self._remove_overlapping_speech(segments)
            
            logging.info(f"å®Ÿéš›ã®pyannote-audioåˆ†é›¢å®Œäº†: {len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            return segments
            
        except Exception as e:
            logging.error(f"pyannote-audioåˆ†é›¢ã§ã‚¨ãƒ©ãƒ¼: {e}")
            logging.info("ç°¡æ˜“åˆ†é›¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            return self._diarize_simple(audio_path, duration, min_duration, max_speakers, force_num_speakers)
    
    def _diarize_simple(
        self,
        audio_path: Path,
        duration: float,
        min_duration: float,
        max_speakers: Optional[int],
        force_num_speakers: Optional[int] = None
    ) -> List[SpeakerSegment]:
        """
        ç°¡æ˜“è©±è€…åˆ†é›¢å®Ÿè£…
        
        Args:
            audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            duration: éŸ³å£°ã®é•·ã•
            min_duration: æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·
            max_speakers: æœ€å¤§è©±è€…æ•°
            
        Returns:
            List[SpeakerSegment]: ç°¡æ˜“è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        """
        logging.info("ç°¡æ˜“è©±è€…åˆ†é›¢å®Ÿè¡Œä¸­...")
        
        # éŸ³å£°ã‚’èª­ã¿è¾¼ã‚“ã§æŒ¯å¹…ãƒ™ãƒ¼ã‚¹ã§è©±è€…å¤‰åŒ–ç‚¹ã‚’æ¨å®š
        try:
            audio_data, sample_rate = AudioUtils.load_audio(audio_path)
            
            # æŒ¯å¹…ãƒ™ãƒ¼ã‚¹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            segments = self._segment_by_amplitude(audio_data, sample_rate, duration, min_duration)
            
            # è©±è€…IDå‰²ã‚Šå½“ã¦ï¼ˆç°¡æ˜“çš„ï¼‰
            target_speakers = force_num_speakers if force_num_speakers is not None else max_speakers
            segments = self._assign_speaker_ids(segments, target_speakers)
            
            # å¼·åˆ¶è©±è€…æ•°å‡¦ç†ï¼ˆç°¡æ˜“åˆ†é›¢ç‰ˆï¼‰
            if force_num_speakers is not None:
                unique_speakers = len(set(seg.speaker_id for seg in segments))
                logging.info(f"ç°¡æ˜“åˆ†é›¢: æ¤œå‡ºã•ã‚ŒãŸè©±è€…æ•°: {unique_speakers}äºº, å¼·åˆ¶è©±è€…æ•°: {force_num_speakers}äºº")
                
                if unique_speakers > force_num_speakers:
                    # ç™ºè©±æ™‚é–“ã®é•·ã„ä¸Šä½Näººã«åˆ¶é™
                    logging.info(f"ç°¡æ˜“åˆ†é›¢: è©±è€…æ•°ã‚’{unique_speakers}äººã‹ã‚‰{force_num_speakers}äººã«åˆ¶é™ã—ã¾ã™")
                    
                    # è©±è€…ã”ã¨ã®ç·ç™ºè©±æ™‚é–“ã‚’è¨ˆç®—
                    speaker_durations = {}
                    for segment in segments:
                        if segment.speaker_id not in speaker_durations:
                            speaker_durations[segment.speaker_id] = 0.0
                        speaker_durations[segment.speaker_id] += segment.duration
                    
                    # ç™ºè©±æ™‚é–“ã®é•·ã„é †ã«ã‚½ãƒ¼ãƒˆ
                    top_speakers = sorted(
                        speaker_durations.items(),
                        key=lambda x: x[1],
                        reverse=True
                    )[:force_num_speakers]
                    
                    top_speaker_ids = [speaker_id for speaker_id, _ in top_speakers]
                    
                    # ä¸Šä½è©±è€…ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’ä¿æŒ
                    segments = [seg for seg in segments if seg.speaker_id in top_speaker_ids]
                    
                    logging.info(f"ç°¡æ˜“åˆ†é›¢: å¼·åˆ¶è©±è€…æ•°åˆ¶é™é©ç”¨å®Œäº†: {len(top_speaker_ids)}äººã®è©±è€…ã‚’ä¿æŒ")
            
            logging.info(f"ç°¡æ˜“è©±è€…åˆ†é›¢å®Œäº†: {len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            return segments
            
        except Exception as e:
            logging.warning(f"ç°¡æ˜“åˆ†é›¢ã§ã‚‚ã‚¨ãƒ©ãƒ¼: {e}")
            # æœ€å¾Œã®æ‰‹æ®µï¼šæ™‚é–“ãƒ™ãƒ¼ã‚¹åˆ†å‰²
            return self._diarize_time_based(duration, min_duration, max_speakers)
    
    def _segment_by_amplitude(
        self,
        audio_data: np.ndarray,
        sample_rate: int,
        duration: float,
        min_duration: float
    ) -> List[SpeakerSegment]:
        """
        æŒ¯å¹…ãƒ™ãƒ¼ã‚¹ã§ã®éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        """
        # 1ç§’ã”ã¨ã®RMSè¨ˆç®—
        frame_length = sample_rate  # 1ç§’
        hop_length = frame_length // 2  # 0.5ç§’ãšã¤ã‚¹ãƒ©ã‚¤ãƒ‰
        
        rms_values = []
        for i in range(0, len(audio_data) - frame_length, hop_length):
            frame = audio_data[i:i + frame_length]
            rms = np.sqrt(np.mean(frame ** 2))
            rms_values.append(rms)
        
        # é–¾å€¤ä»¥ä¸Šã®éƒ¨åˆ†ã‚’éŸ³å£°åŒºé–“ã¨ã™ã‚‹
        threshold = np.percentile(rms_values, 30)  # ä¸‹ä½30%ã‚’ç„¡éŸ³ã¨ã—ã¦é™¤å¤–
        
        segments = []
        in_speech = False
        start_time = 0.0
        
        for i, rms in enumerate(rms_values):
            current_time = i * hop_length / sample_rate
            
            if rms > threshold and not in_speech:
                # éŸ³å£°é–‹å§‹
                start_time = current_time
                in_speech = True
            elif rms <= threshold and in_speech:
                # éŸ³å£°çµ‚äº†
                end_time = current_time
                if (end_time - start_time) >= min_duration:
                    segments.append(SpeakerSegment(
                        start_time=start_time,
                        end_time=end_time,
                        speaker_id="TEMP",  # å¾Œã§å‰²ã‚Šå½“ã¦
                        confidence=0.7
                    ))
                in_speech = False
        
        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†
        if in_speech and (duration - start_time) >= min_duration:
            segments.append(SpeakerSegment(
                start_time=start_time,
                end_time=duration,
                speaker_id="TEMP",
                confidence=0.7
            ))
        
        return segments
    
    def _assign_speaker_ids(
        self,
        segments: List[SpeakerSegment],
        max_speakers: Optional[int]
    ) -> List[SpeakerSegment]:
        """
        ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«è©±è€…IDã‚’å‰²ã‚Šå½“ã¦ï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        """
        if not segments:
            return segments
        
        # è©±è€…æ•°ã‚’æ±ºå®š
        if max_speakers is not None:
            num_speakers = min(max_speakers, 3)
        else:
            num_speakers = min(len(segments) // 2 + 1, 3)
        
        speaker_ids = [f"SPEAKER_{i:02d}" for i in range(num_speakers)]
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®é•·ã•ã«åŸºã¥ã„ã¦è©±è€…ã‚’å¾ªç’°çš„ã«å‰²ã‚Šå½“ã¦
        for i, segment in enumerate(segments):
            speaker_index = i % num_speakers
            segment.speaker_id = speaker_ids[speaker_index]
        
        return segments
    
    def _diarize_time_based(
        self,
        duration: float,
        min_duration: float,
        max_speakers: Optional[int]
    ) -> List[SpeakerSegment]:
        """
        æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        """
        logging.info("æ™‚é–“ãƒ™ãƒ¼ã‚¹åˆ†å‰²ã«ã‚ˆã‚‹è©±è€…åˆ†é›¢")
        
        segments = []
        
        # è©±è€…æ•°ã‚’æ±ºå®šï¼ˆæœ€å¤§3äººã€éŸ³å£°é•·ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        if max_speakers is not None:
            num_speakers = min(max_speakers, 3)
        else:
            num_speakers = min(int(duration / 30) + 1, 3)  # 30ç§’ã”ã¨ã«1äººè¿½åŠ ã€æœ€å¤§3äºº
        
        speaker_ids = [f"SPEAKER_{i:02d}" for i in range(num_speakers)]
        
        # æ™‚é–“ã‚’è©±è€…ã§åˆ†å‰²ï¼ˆç°¡æ˜“çš„ãªå®Ÿè£…ï¼‰
        segment_length = 5.0  # 5ç§’ãšã¤ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
        current_time = 0.0
        speaker_index = 0
        
        while current_time < duration:
            end_time = min(current_time + segment_length, duration)
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ãŒæœ€å°é•·ä»¥ä¸Šã®å ´åˆã®ã¿è¿½åŠ 
            if (end_time - current_time) >= min_duration:
                segment = SpeakerSegment(
                    start_time=current_time,
                    end_time=end_time,
                    speaker_id=speaker_ids[speaker_index],
                    confidence=0.6  # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã¯ä¿¡é ¼åº¦ä½ã‚
                )
                segments.append(segment)
            
            current_time = end_time
            speaker_index = (speaker_index + 1) % num_speakers
        
        return segments
    
    def extract_speaker_audio(
        self,
        audio_path: str,
        segments: List[SpeakerSegment],
        output_dir: str,
        create_individual: bool = True,
        create_combined: bool = True,
        naming_style: str = "detailed"  # "simple" or "detailed"
    ) -> Dict[str, List[str]]:
        """
        è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
        
        Args:
            audio_path: å…ƒéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            segments: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            create_individual: å€‹åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹
            create_combined: çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹
            naming_style: ãƒ•ã‚¡ã‚¤ãƒ«å‘½åã‚¹ã‚¿ã‚¤ãƒ« ("simple": segment_001.wav, "detailed": filename_speaker01_seg001_0m15s-0m23s.wav)
            
        Returns:
            Dict[str, List[str]]: è©±è€…IDã”ã¨ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
            
        Raises:
            FileNotFoundError: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            RuntimeError: éŸ³å£°æŠ½å‡ºå‡¦ç†ã«å¤±æ•—ã—ãŸå ´åˆ
        """
        audio_path = Path(audio_path)
        output_dir = Path(output_dir)
        
        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼
        if not AudioUtils.validate_audio_file(audio_path):
            raise FileNotFoundError(f"æœ‰åŠ¹ãªéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
        
        logging.info(f"è©±è€…éŸ³å£°æŠ½å‡ºé–‹å§‹: {audio_path}")
        logging.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
        
        try:
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            audio_data, sample_rate = AudioUtils.load_audio(audio_path)
            
            # å…ƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ™ãƒ¼ã‚¹åã‚’å–å¾—
            base_name = audio_path.stem  # æ‹¡å¼µå­ãªã—ã®ãƒ•ã‚¡ã‚¤ãƒ«å
            
            # è©±è€…ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            speaker_segments = {}
            for segment in segments:
                if segment.speaker_id not in speaker_segments:
                    speaker_segments[segment.speaker_id] = []
                speaker_segments[segment.speaker_id].append(segment)
            
            output_files = {}
            
            for speaker_id, speaker_segs in speaker_segments.items():
                logging.info(f"è©±è€…{speaker_id}ã®éŸ³å£°æŠ½å‡º: {len(speaker_segs)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
                
                # è©±è€…ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆï¼ˆnaming_styleã«å¿œã˜ã¦ï¼‰
                if naming_style == "detailed":
                    speaker_num = speaker_id.replace("SPEAKER_", "")
                    try:
                        speaker_num = f"{int(speaker_num)+1:02d}"
                    except ValueError:
                        speaker_num = "01"
                    speaker_dir = output_dir / f"speaker_{speaker_num}_{base_name}"
                else:
                    speaker_dir = output_dir / f"speaker_{speaker_id}"
                
                FileUtils.ensure_directory(speaker_dir)
                
                output_files[speaker_id] = []
                extracted_segments = []
                
                # å€‹åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                if create_individual:
                    for i, segment in enumerate(speaker_segs):
                        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’æ™‚é–“ã§åˆ‡ã‚Šå‡ºã—
                        segment_audio = AudioUtils.split_audio_by_time(
                            audio_data, sample_rate,
                            segment.start_time, segment.end_time
                        )
                        
                        if len(segment_audio) > 0:
                            # ãƒ•ã‚§ãƒ¼ãƒ‰å‡¦ç†é©ç”¨
                            segment_audio = AudioUtils.apply_fade(segment_audio, sample_rate)
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
                            filename = self._generate_filename(
                                base_name=base_name,
                                speaker_id=speaker_id,
                                naming_style=naming_style,
                                segment_idx=i+1,
                                start_time=segment.start_time,
                                end_time=segment.end_time
                            )
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                            segment_file = speaker_dir / filename
                            AudioUtils.save_audio(segment_audio, segment_file, sample_rate)
                            output_files[speaker_id].append(str(segment_file))
                            extracted_segments.append(segment_audio)
                
                # çµåˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                if create_combined and extracted_segments:
                    combined_audio = AudioUtils.concatenate_audio(extracted_segments)
                    combined_audio = AudioUtils.normalize_audio(combined_audio)
                    
                    # çµåˆãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
                    combined_filename = self._generate_filename(
                        base_name=base_name,
                        speaker_id=speaker_id,
                        naming_style=naming_style
                    )
                    
                    combined_file = speaker_dir / combined_filename
                    AudioUtils.save_audio(combined_audio, combined_file, sample_rate)
                    output_files[speaker_id].append(str(combined_file))
                
                total_duration = sum(seg.duration for seg in speaker_segs)
                logging.info(f"è©±è€…{speaker_id}æŠ½å‡ºå®Œäº†: åˆè¨ˆ{total_duration:.2f}ç§’")
            
            logging.info(f"å…¨è©±è€…éŸ³å£°æŠ½å‡ºå®Œäº†: {len(speaker_segments)}äºº")
            return output_files
            
        except Exception as e:
            logging.error(f"è©±è€…éŸ³å£°æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {e}")
            raise RuntimeError(f"è©±è€…éŸ³å£°æŠ½å‡ºã«å¤±æ•—: {e}")
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
        
        Returns:
            Dict[str, Any]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        """
        model_info = self.AVAILABLE_MODELS.get(self.model_name, {})
        
        return {
            'model_name': self.model_name,
            'device': self.device,
            'use_auth_token': self.use_auth_token,
            'description': model_info.get('description', ''),
            'language_support': model_info.get('language_support', ''),
            'quality': model_info.get('quality', ''),
            'speed': model_info.get('speed', ''),
            'is_initialized': self._is_initialized
        }
    
    @classmethod
    def list_available_models(cls) -> Dict[str, Dict[str, str]]:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—
        
        Returns:
            Dict[str, Dict[str, str]]: ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¾æ›¸
        """
        return cls.AVAILABLE_MODELS.copy()
    
    def estimate_processing_time(self, audio_duration: float) -> float:
        """
        å‡¦ç†æ™‚é–“ã‚’æ¨å®š
        
        Args:
            audio_duration: éŸ³å£°ã®é•·ã•ï¼ˆç§’ï¼‰
            
        Returns:
            float: æ¨å®šå‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
        """
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å‡¦ç†é€Ÿåº¦ä¿‚æ•°
        speed_factors = {
            'pyannote/speaker-diarization-3.1': 1.5,  # éŸ³å£°é•·ã®1.5å€ç¨‹åº¦
            'pyannote/speaker-diarization': 1.2,      # éŸ³å£°é•·ã®1.2å€ç¨‹åº¦
        }
        
        base_factor = speed_factors.get(self.model_name, 1.5)
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ã‚ˆã‚‹è£œæ­£
        if self.device == 'cpu':
            device_factor = 3.0  # CPUã¯3å€é…ã„
        else:
            device_factor = 1.0  # GPUä½¿ç”¨
        
        estimated_time = audio_duration * base_factor * device_factor
        
        return estimated_time
    
    def analyze_speakers(self, segments: List[SpeakerSegment]) -> Dict[str, Any]:
        """
        è©±è€…åˆ†æçµæœã‚’å–å¾—
        
        Args:
            segments: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
            
        Returns:
            Dict[str, Any]: åˆ†æçµæœ
        """
        if not segments:
            return {
                'num_speakers': 0,
                'total_duration': 0.0,
                'speakers': {}
            }
        
        # è©±è€…ã”ã¨ã®çµ±è¨ˆ
        speaker_stats = {}
        for segment in segments:
            if segment.speaker_id not in speaker_stats:
                speaker_stats[segment.speaker_id] = {
                    'segments': 0,
                    'total_duration': 0.0,
                    'avg_confidence': 0.0,
                    'min_confidence': 1.0,
                    'max_confidence': 0.0
                }
            
            stats = speaker_stats[segment.speaker_id]
            stats['segments'] += 1
            stats['total_duration'] += segment.duration
            stats['avg_confidence'] += segment.confidence
            stats['min_confidence'] = min(stats['min_confidence'], segment.confidence)
            stats['max_confidence'] = max(stats['max_confidence'], segment.confidence)
        
        # å¹³å‡ä¿¡é ¼åº¦è¨ˆç®—
        for speaker_id, stats in speaker_stats.items():
            stats['avg_confidence'] /= stats['segments']
        
        total_duration = sum(seg.duration for seg in segments)
        
        return {
            'num_speakers': len(speaker_stats),
            'total_duration': total_duration,
            'num_segments': len(segments),
            'speakers': speaker_stats
        }
    
    def _remove_overlapping_speech(self, segments: List[SpeakerSegment]) -> List[SpeakerSegment]:
        """
        é‡è¤‡ã™ã‚‹è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ã¾ãŸã¯èª¿æ•´ã™ã‚‹
        
        Args:
            segments: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            
        Returns:
            List[SpeakerSegment]: é‡è¤‡ã‚’é™¤å»ã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        if not segments:
            return segments
        
        # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_segments = sorted(segments, key=lambda x: x.start_time)
        filtered_segments = []
        
        overlap_threshold = 0.1  # 0.1ç§’ä»¥ä¸Šã®é‡è¤‡ã¯é™¤å»å¯¾è±¡
        removed_overlaps = 0
        
        for current_seg in sorted_segments:
            # æ—¢å­˜ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            overlapping = False
            
            for existing_seg in filtered_segments:
                # é‡è¤‡åˆ¤å®š
                overlap_start = max(current_seg.start_time, existing_seg.start_time)
                overlap_end = min(current_seg.end_time, existing_seg.end_time)
                overlap_duration = max(0, overlap_end - overlap_start)
                
                # é‡è¤‡æ™‚é–“ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆ
                if overlap_duration > overlap_threshold:
                    # ã‚ˆã‚Šé•·ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å„ªå…ˆã€åŒã˜é•·ã•ãªã‚‰æ—¢å­˜ã‚’å„ªå…ˆ
                    current_duration = current_seg.duration
                    existing_duration = existing_seg.duration
                    
                    if current_duration > existing_duration:
                        # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æ–¹ãŒé•·ã„å ´åˆã€æ—¢å­˜ã‚’å‰Šé™¤
                        filtered_segments.remove(existing_seg)
                        logging.debug(f"é‡è¤‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»: {existing_seg.speaker_id} {existing_seg.start_time:.2f}-{existing_seg.end_time:.2f}sï¼ˆã‚ˆã‚ŠçŸ­ã„ï¼‰")
                        removed_overlaps += 1
                        break
                    else:
                        # æ—¢å­˜ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å„ªå…ˆã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–
                        overlapping = True
                        logging.debug(f"é‡è¤‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»: {current_seg.speaker_id} {current_seg.start_time:.2f}-{current_seg.end_time:.2f}sï¼ˆé‡è¤‡ï¼‰")
                        removed_overlaps += 1
                        break
            
            # é‡è¤‡ã—ã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
            if not overlapping:
                filtered_segments.append(current_seg)
        
        if removed_overlaps > 0:
            logging.info(f"é‡è¤‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé™¤å»å®Œäº†: {removed_overlaps}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»")
        
        # å†åº¦æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        return sorted(filtered_segments, key=lambda x: x.start_time)
    
    def _force_split_speakers(self, segments: List[SpeakerSegment], target_num_speakers: int) -> List[SpeakerSegment]:
        """
        1ã¤ã®è©±è€…ã¨ã—ã¦æ¤œå‡ºã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å¼·åˆ¶çš„ã«è¤‡æ•°è©±è€…ã«åˆ†å‰²
        
        Args:
            segments: è©±è€…ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
            target_num_speakers: ç›®æ¨™è©±è€…æ•°
            
        Returns:
            List[SpeakerSegment]: åˆ†å‰²ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        """
        if not segments or target_num_speakers <= 1:
            return segments
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_segments = sorted(segments, key=lambda x: x.start_time)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‡ç­‰ã«åˆ†å‰²ã—ã¦ãã‚Œãã‚Œã«ç•°ãªã‚‹è©±è€…IDã‚’å‰²ã‚Šå½“ã¦
        total_segments = len(sorted_segments)
        segments_per_speaker = total_segments // target_num_speakers
        remainder = total_segments % target_num_speakers
        
        result_segments = []
        current_index = 0
        
        for speaker_idx in range(target_num_speakers):
            # ã“ã®è©±è€…ãŒæ‹…å½“ã™ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
            current_count = segments_per_speaker + (1 if speaker_idx < remainder else 0)
            
            # æ–°ã—ã„è©±è€…IDã‚’ç”Ÿæˆ
            new_speaker_id = f"SPEAKER_{speaker_idx:02d}"
            
            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«æ–°ã—ã„è©±è€…IDã‚’å‰²ã‚Šå½“ã¦
            for i in range(current_count):
                if current_index < total_segments:
                    original_seg = sorted_segments[current_index]
                    new_segment = SpeakerSegment(
                        start_time=original_seg.start_time,
                        end_time=original_seg.end_time,
                        speaker_id=new_speaker_id,
                        confidence=original_seg.confidence
                    )
                    result_segments.append(new_segment)
                    current_index += 1
            
            logging.info(f"å¼·åˆ¶åˆ†å‰²: {new_speaker_id} ã« {current_count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’å‰²ã‚Šå½“ã¦")
        
        return result_segments
    
    def _format_time_for_filename(self, seconds: float) -> str:
        """
        ç§’æ•°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®æ™‚é–“æ–‡å­—åˆ—ã«å¤‰æ›
        
        Args:
            seconds: ç§’æ•°
            
        Returns:
            str: "1m23s" å½¢å¼ã®æ™‚é–“æ–‡å­—åˆ—
        """
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes}m{secs:02d}s"
    
    def separate_speakers(
        self,
        input_file: Path,
        output_dir: Path,
        min_segment_length: float = 1.0,
        max_segment_length: float = 30.0,
        overlap_ratio: float = 0.1,
        create_combined: bool = True,
        create_individual: bool = True,
        naming_style: str = "detailed",
        progress_callback: Optional[Callable[[float], None]] = None,
        **kwargs  # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹
    ) -> Dict[str, Any]:
        """
        è©±è€…åˆ†é›¢å‡¦ç†ã‚’å®Ÿè¡Œï¼ˆGUIç”¨ã®çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_file: å…¥åŠ›éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            min_segment_length: æœ€å°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆç§’ï¼‰
            max_segment_length: æœ€å¤§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·ï¼ˆç§’ï¼‰
            overlap_ratio: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”ç‡
            create_combined: çµåˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹
            create_individual: å€‹åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹ã‹
            naming_style: ãƒ•ã‚¡ã‚¤ãƒ«å‘½åã‚¹ã‚¿ã‚¤ãƒ«
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            
        Returns:
            Dict[str, Any]: å‡¦ç†çµæœæƒ…å ±
        """
        try:
            # é€²æ—é€šçŸ¥
            if progress_callback:
                progress_callback(0.0)
            
            # GUIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ä½¿ç”¨ï¼‰
            clustering_threshold = kwargs.get('clustering_threshold', 0.5)
            segmentation_onset = kwargs.get('segmentation_onset', 0.3)
            segmentation_offset = kwargs.get('segmentation_offset', 0.3)
            force_num_speakers = kwargs.get('force_num_speakers', None)
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼šå—ã‘å–ã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèª
            logging.info(f"SpeakerProcessorå—ã‘å–ã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            logging.info(f"  force_num_speakers: {force_num_speakers} (å‹: {type(force_num_speakers)})")
            logging.info(f"  kwargså…¨ä½“: {kwargs}")
            
            # è©±è€…åˆ†é›¢å®Ÿè¡Œ
            segments = self.diarize(
                audio_path=str(input_file),
                min_duration=min_segment_length,
                max_speakers=None,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªå‹•æ¤œå‡º
                clustering_threshold=clustering_threshold,
                segmentation_onset=segmentation_onset,
                segmentation_offset=segmentation_offset,
                force_num_speakers=force_num_speakers
            )
            
            # é€²æ—é€šçŸ¥
            if progress_callback:
                progress_callback(50.0)
            
            # éŸ³å£°æŠ½å‡º
            output_files = self.extract_speaker_audio(
                audio_path=str(input_file),
                segments=segments,
                output_dir=str(output_dir),
                create_individual=create_individual,
                create_combined=create_combined,
                naming_style=naming_style
            )
            
            # é€²æ—é€šçŸ¥
            if progress_callback:
                progress_callback(100.0)
            
            # å‡¦ç†çµæœã‚’ä½œæˆ
            result = {
                'output_files': output_files,
                'segments_detected': len(segments),
                'speakers_detected': len(output_files),
                'total_duration': sum(seg.duration for seg in segments)
            }
            
            logging.info(f"è©±è€…åˆ†é›¢å‡¦ç†å®Œäº†: {len(output_files)}äººã®è©±è€…ã€{len(segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            return result
            
        except Exception as e:
            logging.error(f"è©±è€…åˆ†é›¢å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _generate_filename(
        self, 
        base_name: str, 
        speaker_id: str, 
        naming_style: str,
        segment_idx: Optional[int] = None,
        start_time: Optional[float] = None,
        end_time: Optional[float] = None
    ) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        
        Args:
            base_name: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
            speaker_id: è©±è€…ID
            naming_style: å‘½åã‚¹ã‚¿ã‚¤ãƒ« ("simple" or "detailed")
            segment_idx: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç•ªå·
            start_time: é–‹å§‹æ™‚é–“ï¼ˆç§’ï¼‰
            end_time: çµ‚äº†æ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
        """
        if naming_style == "simple":
            if segment_idx is not None:
                return f"segment_{segment_idx:03d}.wav"
            else:
                return f"speaker_{speaker_id}_combined.wav"
        
        elif naming_style == "detailed":
            # è©±è€…IDã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆSPEAKER_00 -> 01ï¼‰
            speaker_num = speaker_id.replace("SPEAKER_", "")
            try:
                speaker_num = f"{int(speaker_num)+1:02d}"
            except ValueError:
                speaker_num = "01"
            
            if segment_idx is not None:
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
                start_str = self._format_time_for_filename(start_time)
                end_str = self._format_time_for_filename(end_time)
                return f"{base_name}_speaker{speaker_num}_seg{segment_idx:03d}_{start_str}-{end_str}.wav"
            else:
                # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«
                return f"{base_name}_speaker{speaker_num}_combined.wav"
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯simple
            return self._generate_filename(base_name, speaker_id, "simple", segment_idx, start_time, end_time)